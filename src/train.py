import numpy as np
import evaluate
from datasets import load_from_disk
from transformers import (
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification,
    AutoTokenizer
)
from src.data_processor import prepare_datasets, MODEL_CHECKPOINT, LABEL_LIST

def compute_metrics(p):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ (Precision, Recall, F1) –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫—É seqeval (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è NER).
    """
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # –£–±–∏—Ä–∞–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ —Ç–æ–∫–µ–Ω—ã (-100)
    true_predictions = [
        [LABEL_LIST[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [LABEL_LIST[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    metric = evaluate.load("seqeval")
    results = metric.compute(predictions=true_predictions, references=true_labels)
    
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

def main():
    print("1. –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ...")
    tokenized_datasets = prepare_datasets()

    print("\n--- SANITY CHECK –î–ê–ù–ù–´–• ---")
    labels = [item for sublist in tokenized_datasets["train"]["labels"] for item in sublist]
    b_prod_count = labels.count(1) # 1 = B-PROD
    i_prod_count = labels.count(2) # 2 = I-PROD
    total_tokens = len(labels)
    
    print(f"–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
    print(f"–¢–µ–≥–æ–≤ B-PROD (–ù–∞—á–∞–ª–æ —Ç–æ–≤–∞—Ä–∞): {b_prod_count}")
    print(f"–¢–µ–≥–æ–≤ I-PROD (–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ): {i_prod_count}")
    
    if b_prod_count == 0:
        print("üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –í –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞! –ü—Ä–æ–≤–µ—Ä—å data_processor.py")
        return # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∫—Ä–∏–ø—Ç, –Ω–µ—Ç —Å–º—ã—Å–ª–∞ —É—á–∏—Ç—å
    else:
        print(f"üü¢ –î–∞–Ω–Ω—ã–µ –µ—Å—Ç—å. –î–æ–ª—è —Ç–æ–≤–∞—Ä–æ–≤: {((b_prod_count + i_prod_count) / total_tokens):.2%}")
    print("-----------------------------\n")
    
    
    print("2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
    model = AutoModelForTokenClassification.from_pretrained(
        MODEL_CHECKPOINT, 
        num_labels=len(LABEL_LIST),
        id2label={i: l for i, l in enumerate(LABEL_LIST)},
        label2id={l: i for i, l in enumerate(LABEL_LIST)},
    )
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –°–Æ–î–ê, —ç—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    args = TrainingArguments(
        output_dir="models/checkpoint",
        eval_strategy="epoch",       # <--- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ ‚Ññ1 (–Ω–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–∞)
        learning_rate=1e-4,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=15,
        weight_decay=0.005,
        save_strategy="no",
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        # tokenizer=tokenizer,      # <--- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ ‚Ññ2: –£–î–ê–õ–ò–õ–ò –≠–¢–£ –°–¢–†–û–ö–£ (–æ–Ω–∞ –≤—ã–∑—ã–≤–∞–µ—Ç –æ—à–∏–±–∫—É)
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    print("3. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ! (–ú–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 10-20 –º–∏–Ω—É—Ç)...")
    trainer.train()

    print("4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å...")
    # –ú—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤—Ä—É—á–Ω—É—é, —Ç–∞–∫ –Ω–∞–¥–µ–∂–Ω–µ–µ
    model.save_pretrained("models/final_model")
    tokenizer.save_pretrained("models/final_model")
    print("–ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ models/final_model")

if __name__ == "__main__":
    main()